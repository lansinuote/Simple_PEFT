{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfab5d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=10, bias=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from functions import get_loader, get_model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "_, _, loader = get_loader()\n",
    "model, _, _ = get_model()\n",
    "\n",
    "#保存原模型参数\n",
    "model.save_pretrained('model/save_pretrained')\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f09fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,914 || all params: 251,267,508 || trainable%: 0.005537524573213023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModulesToSaveWrapper(\n",
       "  (original_module): lora.Linear(\n",
       "    (base_layer): Linear(in_features=768, out_features=10, bias=True)\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "  )\n",
       "  (modules_to_save): ModuleDict(\n",
       "    (default): lora.Linear(\n",
       "      (base_layer): Linear(in_features=768, out_features=10, bias=True)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, LoftQConfig\n",
    "\n",
    "#lora就是插入一些胶水层,来影响原模型的计算结果\n",
    "#比如原来有一层的计算过程如下:\n",
    "#[b, 1024] -> [b, 1024]\n",
    "#加入lora后变成如下:\n",
    "#[b, 1024] -> [b, 8] -> [b, 1024] -> [b, 1024]\n",
    "#中间的两层就是lora层, 这些层可以在前面,中间或者后面.\n",
    "#这中间的8, 就是下面的参数r\n",
    "#训练时只训练这些lora层,从而缩小计算量,经过实验论证,也是能得到比较好的结果的\n",
    "\n",
    "config = LoraConfig(\n",
    "    #任务类型, SEQ_CLS,SEQ_2_SEQ_LM,CAUSAL_LM,TOKEN_CLS,QUESTION_ANS,FEATURE_EXTRACTION\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    #是否是推理模式.\n",
    "    inference_mode=False,\n",
    "    #降秩矩阵的尺寸,这个参数会影响训练的参数量\n",
    "    r=8,\n",
    "    #降秩矩阵的缩放系数,不影响参数量\n",
    "    lora_alpha=32,\n",
    "    #降秩矩阵的dropout\n",
    "    lora_dropout=0.1,\n",
    "    #指定要对原模型中的那一部分添加lora层,默认是所有位置\n",
    "    target_modules=['classifier'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13b6cd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 62 2.336458683013916 0.03125\n",
      "1 62 2.335559368133545 0.03125\n",
      "2 62 2.3020806312561035 0.125\n",
      "3 62 2.3629519939422607 0.15625\n",
      "4 62 2.3952343463897705 0.09375\n",
      "5 62 2.3703486919403076 0.03125\n",
      "6 62 2.3296892642974854 0.09375\n",
      "7 62 2.2450196743011475 0.15625\n",
      "8 62 2.204324722290039 0.25\n",
      "9 62 2.306922197341919 0.125\n",
      "10 62 2.308375597000122 0.0625\n",
      "11 62 2.3042044639587402 0.125\n",
      "12 62 2.22579026222229 0.25\n",
      "13 62 2.248403549194336 0.15625\n",
      "14 62 2.2131221294403076 0.25\n",
      "15 62 2.1622021198272705 0.25\n",
      "16 62 2.1667230129241943 0.1875\n",
      "17 62 2.12541127204895 0.28125\n",
      "18 62 2.1942343711853027 0.21875\n",
      "19 62 2.086634874343872 0.34375\n",
      "20 62 2.1510114669799805 0.25\n",
      "21 62 2.159356117248535 0.375\n",
      "22 62 2.141340732574463 0.34375\n",
      "23 62 2.131850481033325 0.34375\n",
      "24 62 2.1174545288085938 0.21875\n",
      "25 62 2.087817668914795 0.53125\n",
      "26 62 2.1108903884887695 0.375\n",
      "27 62 2.0811893939971924 0.46875\n",
      "28 62 2.055372953414917 0.40625\n",
      "29 62 2.0447239875793457 0.46875\n",
      "30 62 2.0550787448883057 0.34375\n",
      "31 62 2.0166432857513428 0.53125\n",
      "32 62 1.993998408317566 0.4375\n",
      "33 62 2.0733022689819336 0.4375\n",
      "34 62 2.0332281589508057 0.5\n",
      "35 62 2.022028923034668 0.5\n",
      "36 62 2.0159997940063477 0.46875\n",
      "37 62 1.9176312685012817 0.78125\n",
      "38 62 1.942380428314209 0.625\n",
      "39 62 1.9386800527572632 0.5625\n",
      "40 62 1.9307005405426025 0.65625\n",
      "41 62 1.9886863231658936 0.59375\n",
      "42 62 1.9458670616149902 0.5625\n",
      "43 62 1.928817868232727 0.71875\n",
      "44 62 1.8715364933013916 0.78125\n",
      "45 62 1.873715877532959 0.65625\n",
      "46 62 1.8763353824615479 0.6875\n",
      "47 62 1.86095130443573 0.75\n",
      "48 62 1.948620319366455 0.59375\n",
      "49 62 1.8806779384613037 0.8125\n",
      "50 62 1.8373451232910156 0.75\n",
      "51 62 1.842321753501892 0.78125\n",
      "52 62 1.8271459341049194 0.90625\n",
      "53 62 1.8349123001098633 0.84375\n",
      "54 62 1.8083330392837524 0.875\n",
      "55 62 1.8403195142745972 0.8125\n",
      "56 62 1.749794840812683 0.96875\n",
      "57 62 1.755589485168457 0.90625\n",
      "58 62 1.7760461568832397 0.90625\n",
      "59 62 1.736643671989441 0.84375\n",
      "60 62 1.76534104347229 0.9375\n",
      "61 62 1.723689079284668 0.90625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=12, microseconds=194751)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "#正常训练\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.to(device)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "for i, data in enumerate(loader):\n",
    "    for k, v in data.items():\n",
    "        data[k] = v.to(device)\n",
    "    out = model(**data)\n",
    "    out.loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        labels = data['labels']\n",
    "        logits = out['logits'].argmax(1)\n",
    "        acc = (labels == logits).sum().item() / len(labels)\n",
    "\n",
    "        print(i, len(loader), out.loss.item(), acc)\n",
    "\n",
    "datetime.datetime.now() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e6f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cuda117/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0019, -0.0047,  0.0170,  ...,  0.0103, -0.0181, -0.0162],\n",
       "        [ 0.0281,  0.0129,  0.0396,  ..., -0.0123,  0.0515, -0.0117],\n",
       "        [-0.0530, -0.0161, -0.0173,  ..., -0.0548,  0.0034, -0.0369],\n",
       "        ...,\n",
       "        [-0.0228, -0.0049,  0.0235,  ..., -0.0174,  0.0303,  0.0107],\n",
       "        [-0.0392,  0.0481,  0.0245,  ...,  0.0204, -0.0020,  0.0287],\n",
       "        [ 0.0116, -0.0089, -0.0318,  ...,  0.0126, -0.0058, -0.0059]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#peft保存,保存的文件会很小,因为只保存了lora层\n",
    "model.save_pretrained('model/peft.save_pretrained')\n",
    "\n",
    "model.base_model.classifier.modules_to_save.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17d9ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0019, -0.0047,  0.0170,  ...,  0.0103, -0.0181, -0.0162],\n",
       "        [ 0.0281,  0.0129,  0.0396,  ..., -0.0123,  0.0515, -0.0117],\n",
       "        [-0.0530, -0.0161, -0.0173,  ..., -0.0548,  0.0034, -0.0369],\n",
       "        ...,\n",
       "        [-0.0228, -0.0049,  0.0235,  ..., -0.0174,  0.0303,  0.0107],\n",
       "        [-0.0392,  0.0481,  0.0245,  ...,  0.0204, -0.0020,  0.0287],\n",
       "        [ 0.0116, -0.0089, -0.0318,  ...,  0.0126, -0.0058, -0.0059]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "#重启初始化原模型\n",
    "model = BertForSequenceClassification.from_pretrained('model/save_pretrained')\n",
    "\n",
    "#加载保存的config\n",
    "PeftConfig.from_pretrained('model/peft.save_pretrained')\n",
    "\n",
    "#插入保存的lora层\n",
    "model = PeftModel.from_pretrained(model,\n",
    "                                  './model/peft.save_pretrained',\n",
    "                                  is_trainable=True)\n",
    "\n",
    "model.base_model.classifier.modules_to_save.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f548b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试模型性能\n",
    "def test(model):\n",
    "    model.to(device)\n",
    "    data = next(iter(loader))\n",
    "    for k, v in data.items():\n",
    "        data[k] = v.to(device)\n",
    "    with torch.no_grad():\n",
    "        outs = model(**data)\n",
    "    acc = (outs.logits.argmax(1) == data.labels).sum().item() / len(\n",
    "        data.labels)\n",
    "    return acc\n",
    "\n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63cefec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.bert.modeling_bert.BertForSequenceClassification, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#合并lora层到原始模型中,效果不会改变\n",
    "model_merge = model.merge_and_unload()\n",
    "\n",
    "type(model_merge), test(model_merge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
