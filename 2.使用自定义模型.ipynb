{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97221f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=10, bias=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from functions import get_loader, get_model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "_, _, loader = get_loader()\n",
    "model, _, _ = get_model()\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdc909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 252,033,802 || trainable%: 0.31203433577532586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=10, bias=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, LoftQConfig\n",
    "\n",
    "#此处不再指定task_type\n",
    "config = LoraConfig(inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79902e1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 62 2.2774503231048584 0.125\n",
      "1 62 2.2287027835845947 0.1875\n",
      "2 62 2.3163657188415527 0.15625\n",
      "3 62 2.209228992462158 0.21875\n",
      "4 62 2.1856229305267334 0.15625\n",
      "5 62 2.2188992500305176 0.1875\n",
      "6 62 2.0847771167755127 0.34375\n",
      "7 62 2.1538450717926025 0.3125\n",
      "8 62 2.0640904903411865 0.375\n",
      "9 62 1.9859848022460938 0.5625\n",
      "10 62 1.953343152999878 0.59375\n",
      "11 62 1.9733211994171143 0.6875\n",
      "12 62 1.9323341846466064 0.6875\n",
      "13 62 1.8512815237045288 0.78125\n",
      "14 62 1.9471282958984375 0.625\n",
      "15 62 1.7356795072555542 0.90625\n",
      "16 62 1.759588599205017 0.875\n",
      "17 62 1.6408013105392456 0.96875\n",
      "18 62 1.7141221761703491 0.8125\n",
      "19 62 1.6078336238861084 0.9375\n",
      "20 62 1.5887126922607422 1.0\n",
      "21 62 1.4986778497695923 0.96875\n",
      "22 62 1.5481253862380981 0.875\n",
      "23 62 1.4178872108459473 1.0\n",
      "24 62 1.371886968612671 1.0\n",
      "25 62 1.4725046157836914 0.9375\n",
      "26 62 1.3218789100646973 0.96875\n",
      "27 62 1.359554409980774 1.0\n",
      "28 62 1.2736034393310547 0.96875\n",
      "29 62 1.2936562299728394 0.96875\n",
      "30 62 1.1820000410079956 1.0\n",
      "31 62 1.1500431299209595 1.0\n",
      "32 62 1.122283697128296 1.0\n",
      "33 62 1.097461462020874 1.0\n",
      "34 62 0.9414330720901489 1.0\n",
      "35 62 1.0303120613098145 1.0\n",
      "36 62 0.9005396366119385 1.0\n",
      "37 62 0.8811569213867188 1.0\n",
      "38 62 0.8738059997558594 1.0\n",
      "39 62 0.8506068587303162 1.0\n",
      "40 62 0.7993475198745728 1.0\n",
      "41 62 0.7811093926429749 1.0\n",
      "42 62 0.7609606981277466 1.0\n",
      "43 62 0.759360671043396 1.0\n",
      "44 62 0.6664047241210938 1.0\n",
      "45 62 0.660634458065033 1.0\n",
      "46 62 0.6855562925338745 1.0\n",
      "47 62 0.6103322505950928 1.0\n",
      "48 62 0.5793362855911255 1.0\n",
      "49 62 0.5576601624488831 1.0\n",
      "50 62 0.5652727484703064 1.0\n",
      "51 62 0.5442566275596619 1.0\n",
      "52 62 0.5221884250640869 1.0\n",
      "53 62 0.4475937783718109 1.0\n",
      "54 62 0.5214552879333496 1.0\n",
      "55 62 0.48425009846687317 1.0\n",
      "56 62 0.3857068419456482 1.0\n",
      "57 62 0.4370235204696655 1.0\n",
      "58 62 0.4066462218761444 1.0\n",
      "59 62 0.39984583854675293 1.0\n",
      "60 62 0.3576478660106659 1.0\n",
      "61 62 0.36198505759239197 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=25, microseconds=576708)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "#正常训练\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.to(device)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "for i, data in enumerate(loader):\n",
    "    for k, v in data.items():\n",
    "        data[k] = v.to(device)\n",
    "    out = model(**data)\n",
    "    out.loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        labels = data['labels']\n",
    "        logits = out['logits'].argmax(1)\n",
    "        acc = (labels == logits).sum().item() / len(labels)\n",
    "\n",
    "        print(i, len(loader), out.loss.item(), acc)\n",
    "\n",
    "datetime.datetime.now() - now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
